# 待处理问题                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       

1. TTA
2. 在statistics中处理evaluate目录问题（不同pc目录不同），进行自动转换
3. 编写一个程序，将所有标签均在50%之后的模型罗列出来，进行删除
4. 每次训练的时候统计概率分布图像（显示上采样、下采样的效果）
5. 通过图片显示模型的相关性（使用阈值分割为0，1之后，求相同值）
6. 验证blue -> red的效果（已在densenet201上验证了）
7. init statge bug（如果第一阶段多余1个epoch，那么init_epoch=1时，不会触发加载权重）
8. 试一试batch size 16
9. 尝试不同尺寸的图片（能够支持不同阶段不同尺寸）
10. stacking：xgboost、lightgbm、NN、逻辑回归、ridge回归
11. greedy f2和 smooth f2究竟哪个更能代表泛化能力？最终提交的时候两个都要试一下。

## 数据不均衡问题

### 特征工程

### 数据扩充

修改服装的颜色（不少服装款式相同，颜色不同）

### coss function

由于标签不均衡，第三个、倒数第五个标签的比例分别为 0.9，0.9，导致学习的过程中，这两个值基本被预测为1，其他的全部倾向于0，如下是使用Original、Segmented数据，训练了30个Epoch后，对Val进行预测的结果：

'my_output': array([[1.73255524e-14, 1.2 9659707e-03, 9 .99820888e-01, 1.13264077e-11,
        8.29777273e-05, 7.33385561e-04, 1.29288066e-10, 3.33989283e-06,
        9.99999642e-01, 1.45883618e-07, 7.95193245e-09, 6.16776527e-14,
        1.08028235e-12],
       [5.44229971e-24, 8.75639955e-07, 9.99993682e-01, 3.19490839e-18,
        3.62233954e-09, 5.08854610e-05, 2.00667231e-19, 7.88219073e-11,
        1.00000000e+00, 3.89001467e-13, 2.58106459e-19, 2.85204262e-21,
        1.46501005e-16],
       [5.82149884e-24, 3.34603919e-06, 9.99999762e-01, 2.74838875e-19,
        1.58705927e-06, 1.84531501e-07, 1.69889613e-17, 8.40869475e-12,
        1.00000000e+00, 2.02899695e-13, 8.78627700e-15, 4.77588761e-22,
        1.85213515e-18],
       [8.67193253e-22, 4.67927239e-05, 9.99993682e-01, 8.73316198e-19,
        6.74705298e-05, 3.18978823e-06, 2.79685578e-15, 4.86659379e-10,
        1.00000000e+00, 1.62092666e-11, 1.89955596e-16, 1.93475667e-21,
        2.95958373e-15],
       [1.09179207e-18, 8.63464447e-05, 9.99956965e-01, 1.27800927e-16,
        2.47708090e-06, 3.35021214e-05, 5.21968102e-13, 2.94599090e-07,
        1.00000000e+00, 1.27435701e-10, 1.18900603e-13, 3.04629966e-19,
        2.98748775e-15],
       [1.30868954e-14, 2.51155780e-05, 9.99815762e-01, 3.56874686e-12,
        5.91327589e-05, 1.60903975e-04, 4.33566946e-13, 4.47841515e-08,
        9.99980330e-01, 1.62867908e-09, 7.31504510e-11, 3.66607948e-13,
        2.99221433e-12],



可能的方案：

1. 权重，根据标签的密度来给每一种标签的loss加权重（使用加权bce作为权重）
2. 标签分开训练，将比例差不多的标签放到一起训练（无法学到标签之间的关联关系）
3. 虽然部分标签的值很小，但是仍然是有变化的，依然可用使用阈值来进行切割。可用试试切割是否准确



咨询了一下Planet Amazon的冠军，他们说如果val数据集和test数据集分布相同，那么不做处理效果更好（他们没有做处理）。

### Weighted BCE Loss

这是我自己想的一种方法，不知道是否有效，从效果来看的确让标签的值变得多样化。但是不清楚，这究竟是正确的还是随机的噪声，等后续将基于权重搜索的F2-Score实现后，看一看最终的效果

1. 根据标签的密度，按照一下权重来对BCE LOSS进行加权（将标签的密度放大到1）

```
527, 12.8, 1.1, 210, 2.8, 6.18, 279.32, 40.5, 1.11, 7.7, 14.79, 43.9, 156
```

```
global steps = 15115

'my_output': array([[2.87615620e-02, 3.60131587e-08, 1.00000000e+00, 9.34116933e-13,
        7.78415084e-01, 1.18359709e-02, 5.13187466e-08, 6.53684959e-02,
        1.00000000e+00, 4.39451336e-10, 2.75671277e-06, 5.23333088e-04,
        2.88969254e-14],
       [9.84157920e-01, 4.70034574e-05, 1.00000000e+00, 5.12703657e-10,
        6.76649809e-01, 2.43684705e-02, 4.05071041e-05, 6.81642354e-01,
        1.00000000e+00, 8.52663551e-10, 2.56278756e-04, 4.89057020e-05,
        5.48346160e-11],
       [3.22679698e-01, 8.49908702e-07, 1.00000000e+00, 5.53154961e-12,
        2.88722247e-01, 4.33327377e-01, 6.45374093e-05, 7.92339742e-01,
        1.00000000e+00, 1.82979916e-11, 2.55159563e-07, 6.24645281e-07,
        5.25569779e-12],
       [8.02957173e-03, 1.26484210e-07, 1.00000000e+00, 6.99199650e-13,
        9.92807746e-01, 1.51375111e-03, 2.38902931e-06, 7.49298215e-01,
        1.00000000e+00, 5.00108066e-10, 6.98621420e-07, 9.11940951e-05,
        9.13620037e-14],
       [4.98848230e-01, 1.14626251e-04, 1.00000000e+00, 3.84805380e-17,
        4.78389939e-06, 9.03646946e-01, 3.59346275e-10, 8.00661892e-02,
        1.00000000e+00, 1.76480235e-08, 3.01785888e-11, 5.12778480e-03,
        6.03238975e-20] 
```

```
global steps = 21985
{'my_output': array([[9.99936819e-01, 2.95100165e-07, 1.00000000e+00, 2.53286229e-11,
        8.42167377e-01, 8.11851919e-01, 1.81431301e-06, 4.42770422e-01,
        1.00000000e+00, 1.35282128e-12, 1.47628310e-09, 6.69168831e-10,
        7.68692218e-22],
       [2.93428987e-01, 3.93031724e-03, 1.00000000e+00, 3.17963600e-19,
        5.91983462e-06, 3.28291953e-01, 1.38492933e-05, 1.82572054e-03,
        1.00000000e+00, 2.86169336e-13, 7.25760632e-11, 3.57592329e-02,
        2.86158307e-25],
       [6.17216349e-01, 2.78287730e-03, 1.00000000e+00, 1.76822532e-12,
        3.25193349e-03, 3.50383285e-04, 5.68054691e-02, 4.42319037e-03,
        1.00000000e+00, 2.92008995e-09, 4.91057150e-08, 2.97096949e-02,
        6.63382720e-14],
       [2.83043346e-06, 9.23963395e-09, 1.00000000e+00, 2.96028035e-10,
        4.52617407e-01, 1.81097828e-04, 3.59331267e-07, 8.44809599e-03,
        1.00000000e+00, 1.14195431e-09, 1.18340167e-05, 1.55193251e-04,
        1.25051159e-15],
       [8.88331890e-01, 1.87989990e-05, 1.00000000e+00, 1.10013967e-13,
        3.47082675e-01, 1.70409694e-01, 2.99944281e-06, 1.59343719e-01,
        1.00000000e+00, 5.07361687e-11, 7.09992121e-10, 3.64392214e-02,
        1.88906778e-19],
       [1.48144827e-04, 2.73992032e-06, 1.00000000e+00, 8.39061893e-13,
        6.21999800e-01, 9.58482027e-01, 3.96175710e-06, 9.56250057e-02,
        1.00000000e+00, 6.08331181e-12, 2.45630669e-11, 9.76795200e-10,
        2.44567903e-20],
       [2.86166113e-09, 3.52901863e-09, 1.00000000e+00, 2.57086441e-08,
        8.20045531e-01, 9.97779071e-01, 1.14259535e-12, 4.96110506e-02,
        1.00000000e+00, 3.89012780e-12, 2.48500481e-02, 9.35924394e-11,
        4.58551036e-19],
```

从上诉的几次预测来看，网络的输出不再是单调的两个标签为1，其他为0. f2-score从0.6逐步上升到0.7（还未做阈值搜素，做了之后可能会有所不同）。





## 集成

1. 降低bias：可用先基于单标签进行一次stacking（同一个模型内部），选出每个标签f2-score前k个最优模型，然后基于多次随机贪心的搜寻方式（或者用beam search），得出一个组合模型的f2-score排序（同一个模型的同一个标签不能出现两次），然后根据这个排序选出前n个输入下一个ensemb layer
2. 降维、降低variance：同一个模型有太多的权重，如InceptionV3 val1就训练了10次。是否可以先进行一次bagging（每种模型选出bagging之后的几个，如两个），然后再输入到下一个ensemble layer
3. 降低variance：original和segmented可用考虑分开做stacking，然后再stacking（segment相当于给original加上了一个注意力模型）



## 性能

当前GTX1080TI，GPU利用率可用跑到98%了，因此暂时没有优化的必要了。



## 图像预处理

判断哪些图像预处理手段是不需要的



目前进行了如下预处理：

+ horizontal flip
+ random rotation
+ high、width random shift



问题：

+ keras的图像预处理是不改变大小的（空白出采用像素生成来填补）
+ 使用动态生成还是静态生成，静态生成的话每个epoch会变得非常长



参考《Imagenet classification with deep convolutional neural networks》、《Deep Residual Learning for Image Recognition》两篇论文中的训练方法：

+ 训练过程中对图片进行随机裁剪（边长的1/8）
+ 在随机裁剪的基础上，随机水平翻转
+ 预测时，将图片进行五次裁剪（四个角落+中心），以及水平翻转。得到10个样本。然后预测取平均
+ PCA Jitter

## 训练

###batch size

在知乎上看到如下观点：

+ 越大越好，塞满显存
+ 不大不小比较好（玄学）
+ 使用了Batch Norm后，batch size需要增大，否则泛化能力不好（？？？？）
+ 前期使用小batch size，后期使用大batch size（不要减小学习率，增大batch size）

### Alex

SGD，学习衰减0.0005，动量0.9，批次128

###ResNet

SGD，初始化0.1，学习衰减0.0001，动量0.9，批次256



### VGG

ConvNet训练过程通常遵循Krizhevsky等人（2012）（除了从多尺度训练图像中对输入裁剪图像进行采样外，如下文所述）。也就是说，通过使用具有动量的小批量梯度下降（基于反向传播（LeCun等人，1989））优化多项式逻辑回归目标函数来进行训练。批量大小设为256，动量为0.9。训练通过权重衰减（L2惩罚乘子设定为$5 * 10^{-4}$）进行正则化，前两个全连接层执行丢弃正则化（丢弃率设定为0.5）。学习率初始设定为0.01，然后当验证集准确率停止改善时，减少10倍。学习率总共降低3次，学习在37万次迭代后停止（74个epochs）。 

在尺寸256上预训练，初始化学习率0.001，然后在384上进一步训练。



### DenseNet

所有网络都使用随机梯度下降(SGD)进行训练。在CIFAR和SVHN上，我们训练批量为64，分别训练300和40个周期。初始学习率设置为0.1，在训练周期数达到50％和75％时除以10。在ImageNet上，训练批量为256，训练90个周期。学习速率最初设置为0.1，并在训练周期数达到30和60时除以10。由于GPU内存限制，我们最大的模型(DenseNet-161)以小批量128进行训练。为了补偿较小的批量，我们训练该模型的周期数为100，并在训练周期数达到90时将学习率除以10。

我们使用的权重衰减为$10^{-4}$ ，Nesterov动量为0.9且没有衰减。对于没有数据增强的三个数据集，即C10，C100和SVHN，我们在每个卷积层之后（除第一个层之外）添加一个Dropout层[37](https://alvinzhu.xyz/2017/10/07/densenet/#fn:32)，并将Dropout率设置为0.2。只对每个任务和超参数设置评估一次测试误差。





## 迁移学习问题

### 权重冻结

逐渐解开权重，和直接训练所有权重的差异